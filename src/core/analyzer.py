"""
Module d'analyse IA pour l'Agent de Cybers√©curit√©

Ce module utilise des mod√®les d'IA (OpenAI GPT, Ollama, etc.) pour analyser
les vuln√©rabilit√©s d√©tect√©es et fournir des recommandations de rem√©diation
intelligentes et prioris√©es.

Fonctionnalit√©s :
- Analyse contextuelle des vuln√©rabilit√©s
- √âvaluation des risques et impacts business
- Priorisation automatique bas√©e sur CVSS et contexte
- G√©n√©ration de plans de rem√©diation
- Support multi-mod√®les IA (OpenAI, Ollama, Anthropic)
"""

import asyncio
import json
import logging
import time
from datetime import datetime
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass, asdict
from pathlib import Path

import openai
import requests
from openai import AsyncOpenAI

from config import get_config, get_llm_config
from config.prompts import (
    format_vulnerability_prompt,
    format_priority_assessment_prompt,
    format_executive_report_prompt
)
from src.utils.logger import setup_logger
from src.database.database import Database
from . import AnalyzerException, CoreErrorCodes, ERROR_MESSAGES

# Configuration du logging
logger = setup_logger(__name__)


# === MOD√àLES DE DONN√âES ===

@dataclass
class VulnerabilityAnalysis:
    """Analyse d'une vuln√©rabilit√© individuelle"""
    vulnerability_id: str
    name: str
    severity: str
    cvss_score: float
    impact_analysis: str
    exploitability: str
    priority_score: int
    affected_service: str
    recommended_actions: List[str]
    dependencies: List[str]
    references: List[str]
    business_impact: Optional[str] = None
    remediation_effort: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        """Convertit en dictionnaire"""
        return asdict(self)


@dataclass
class AnalysisResult:
    """R√©sultat complet d'une analyse IA"""
    analysis_id: str
    target_system: str
    analyzed_at: datetime

    # R√©sum√© de l'analyse
    analysis_summary: Dict[str, Any]

    # Vuln√©rabilit√©s analys√©es
    vulnerabilities: List[VulnerabilityAnalysis]

    # Plan de rem√©diation
    remediation_plan: Dict[str, Any]

    # M√©tadonn√©es
    ai_model_used: str
    confidence_score: float
    processing_time: float

    def to_dict(self) -> Dict[str, Any]:
        """Convertit en dictionnaire"""
        result = asdict(self)
        result['vulnerabilities'] = [vuln.to_dict() for vuln in self.vulnerabilities]
        result['analyzed_at'] = self.analyzed_at.isoformat()
        return result


# === CLASSE PRINCIPALE ===

class Analyzer:
    """
    Analyseur IA de vuln√©rabilit√©s

    Cette classe orchestre l'analyse des vuln√©rabilit√©s en utilisant
    des mod√®les d'IA pour fournir des insights contextuels et des
    recommandations de rem√©diation prioris√©es.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Initialise l'analyseur IA

        Args:
            config: Configuration personnalis√©e (optionnel)
        """
        self.config = config or get_config()
        self.llm_config = get_llm_config("openai")  # Par d√©faut OpenAI

        # √âtat de l'analyseur
        self.is_ready = False
        self.current_provider = None
        self.client = None

        # Base de donn√©es pour historique
        self.db = Database()

        # Statistiques
        self.stats = {
            "total_analyses": 0,
            "successful_analyses": 0,
            "failed_analyses": 0,
            "average_processing_time": 0.0
        }

        # Initialiser le client IA
        self._initialize_ai_client()

    def _initialize_ai_client(self):
        """Initialise le client IA selon la configuration"""
        try:
            provider = self.llm_config.get("provider", "openai")

            if provider == "openai":
                self._setup_openai_client()
            elif provider == "ollama":
                self._setup_ollama_client()
            elif provider == "anthropic":
                self._setup_anthropic_client()
            else:
                raise AnalyzerException(
                    f"Fournisseur IA non support√©: {provider}",
                    CoreErrorCodes.INVALID_CONFIGURATION
                )

            self.current_provider = provider
            self.is_ready = True
            logger.info(f"Client IA initialis√©: {provider}")

        except Exception as e:
            logger.error(f"Erreur initialisation client IA: {e}")
            raise AnalyzerException(
                f"Impossible d'initialiser le client IA: {str(e)}",
                CoreErrorCodes.AI_SERVICE_ERROR
            )

    def _setup_openai_client(self):
        """Configure le client OpenAI"""
        api_key = self.config.openai_api_key
        if not api_key:
            raise AnalyzerException(
                "Cl√© API OpenAI manquante",
                CoreErrorCodes.INVALID_CONFIGURATION
            )

        self.client = AsyncOpenAI(api_key=api_key)
        logger.debug("Client OpenAI configur√©")

    def _setup_ollama_client(self):
        """Configure le client Ollama"""
        base_url = self.llm_config.get("base_url", "http://localhost:11434")

        # V√©rifier que Ollama est accessible
        try:
            response = requests.get(f"{base_url}/api/tags", timeout=5)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            raise AnalyzerException(
                f"Ollama non accessible √† {base_url}: {str(e)}",
                CoreErrorCodes.AI_SERVICE_ERROR
            )

        self.client = {"base_url": base_url, "type": "ollama"}
        logger.debug(f"Client Ollama configur√©: {base_url}")

    def _setup_anthropic_client(self):
        """Configure le client Anthropic"""
        # TODO: Impl√©menter le support Anthropic Claude
        raise AnalyzerException(
            "Support Anthropic pas encore impl√©ment√©",
            CoreErrorCodes.AI_SERVICE_ERROR
        )

    async def analyze_vulnerabilities(
            self,
            vulnerabilities_data: List[Dict[str, Any]],
            target_system: str = "Unknown System",
            business_context: Optional[Dict[str, Any]] = None
    ) -> AnalysisResult:
        """
        Analyse des vuln√©rabilit√©s avec l'IA

        Args:
            vulnerabilities_data: Liste des vuln√©rabilit√©s √† analyser
            target_system: Syst√®me cible pour le contexte
            business_context: Contexte business optionnel

        Returns:
            AnalysisResult: R√©sultats d'analyse complets

        Raises:
            AnalyzerException: Si l'analyse √©choue
        """
        if not self.is_ready:
            raise AnalyzerException(
                "Analyseur non initialis√©",
                CoreErrorCodes.MODULE_NOT_READY
            )

        if not vulnerabilities_data:
            raise AnalyzerException(
                "Aucune donn√©e de vuln√©rabilit√© fournie",
                CoreErrorCodes.INVALID_VULNERABILITY_DATA
            )

        start_time = time.time()
        analysis_id = f"analysis_{int(time.time())}"

        try:
            logger.info(f"D√©but analyse {analysis_id} - {len(vulnerabilities_data)} vuln√©rabilit√©s")

            # AJOUT: Log de debug
            logger.info("üîç √âTAPE 1: Formatage des donn√©es...")
            formatted_data = self._format_vulnerabilities_for_ai(vulnerabilities_data)
            logger.info(f"‚úÖ √âTAPE 1: OK - {len(formatted_data)} caract√®res")

            logger.info("üîç √âTAPE 2: Appel IA...")
            analyzed_vulnerabilities = await self._analyze_individual_vulnerabilities(
                formatted_data, target_system
            )
            logger.info(f"‚úÖ √âTAPE 2: OK - {len(analyzed_vulnerabilities)} vuln√©rabilit√©s")

            # √âtape 2: √âvaluation de priorit√© globale
            remediation_plan = await self._generate_remediation_plan(
                analyzed_vulnerabilities, business_context
            )

            # √âtape 3: G√©n√©ration du r√©sum√© ex√©cutif
            analysis_summary = await self._generate_analysis_summary(
                analyzed_vulnerabilities, remediation_plan
            )

            # Calculer les m√©trics
            processing_time = time.time() - start_time
            confidence_score = self._calculate_confidence_score(analyzed_vulnerabilities)

            # Cr√©er le r√©sultat
            result = AnalysisResult(
                analysis_id=analysis_id,
                target_system=target_system,
                analyzed_at=datetime.utcnow(),
                analysis_summary=analysis_summary,
                vulnerabilities=analyzed_vulnerabilities,
                remediation_plan=remediation_plan,
                ai_model_used=self._get_model_name(),
                confidence_score=confidence_score,
                processing_time=processing_time
            )

            # Sauvegarder dans la base de donn√©es
            await self._save_analysis_result(result)

            # Mettre √† jour les statistiques
            self._update_stats(True, processing_time)

            logger.info(f"Analyse termin√©e: {analysis_id} ({processing_time:.2f}s)")
            return result

        except Exception as e:
            self._update_stats(False, time.time() - start_time)
            logger.error(f"Erreur analyse {analysis_id}: {e}")

            if isinstance(e, AnalyzerException):
                raise
            else:
                raise AnalyzerException(
                    f"Erreur lors de l'analyse: {str(e)}",
                    CoreErrorCodes.ANALYSIS_FAILED
                )

    def _format_vulnerabilities_for_ai(self, vulnerabilities: List[Dict[str, Any]]) -> str:
        """Formate les vuln√©rabilit√©s pour l'IA"""
        formatted = []

        for vuln in vulnerabilities:
            vuln_text = f"""
Vuln√©rabilit√©: {vuln.get('name', 'Unknown')}
CVE: {vuln.get('cve_id', 'N/A')}
Gravit√©: {vuln.get('severity', 'Unknown')}
Score CVSS: {vuln.get('cvss_score', 'N/A')}
Service: {vuln.get('affected_service', 'Unknown')}
Ports: {', '.join(map(str, vuln.get('ports', [])))}
Description: {vuln.get('description', 'Pas de description')}
"""
            formatted.append(vuln_text.strip())

        return "\n\n---\n\n".join(formatted)

    async def _analyze_individual_vulnerabilities(
            self,
            vulnerabilities_data: str,
            target_system: str
    ) -> List[VulnerabilityAnalysis]:
        """Analyse d√©taill√©e de chaque vuln√©rabilit√©"""

        # Pr√©parer le prompt
        prompt = format_vulnerability_prompt(
            os_info=target_system,
            services="Services d√©tect√©s lors du scan",
            open_ports="Ports ouverts d√©tect√©s",
            vulnerabilities_data=vulnerabilities_data
        )

        # Analyser avec l'IA
        ai_response = await self._call_ai_model(prompt)

        # Parser la r√©ponse JSON avec gestion d'erreur robuste
        try:
            logger.debug(f"R√©ponse IA brute (200 premiers chars): {ai_response[:200] if ai_response else 'VIDE'}")

            if not ai_response or not ai_response.strip():
                logger.error("R√©ponse IA vide")
                return self._create_default_analysis(vulnerabilities_data)

            # Nettoyer la r√©ponse
            clean_response = ai_response.strip()

            # Cas 1: R√©ponse avec ```json
            if '```json' in clean_response.lower():
                start = clean_response.lower().find('```json') + 7
                end = clean_response.find('```', start)
                if end > start:
                    clean_response = clean_response[start:end].strip()

            # Cas 2: R√©ponse avec ``` simple
            elif clean_response.startswith('```'):
                start_idx = clean_response.find('{')
                end_idx = clean_response.rfind('}') + 1
                if start_idx != -1 and end_idx > start_idx:
                    clean_response = clean_response[start_idx:end_idx]

            # Cas 3: Trouver juste le JSON si pas de backticks
            elif not clean_response.startswith('{'):
                start_idx = clean_response.find('{')
                end_idx = clean_response.rfind('}') + 1
                if start_idx != -1 and end_idx > start_idx:
                    clean_response = clean_response[start_idx:end_idx]

            logger.debug(f"JSON nettoy√© (200 premiers chars): {clean_response[:200]}")

            # Parser le JSON
            analysis_data = None
            try:
                analysis_data = json.loads(clean_response)
            except json.JSONDecodeError as parse_error:
                logger.error(f"Erreur JSON parsing: {parse_error}")
                logger.error(f"Position: {parse_error.pos}")
                logger.error(f"Ligne: {parse_error.lineno}")

                # Log plus de contexte
                error_context_start = max(0, parse_error.pos - 100)
                error_context_end = min(len(clean_response), parse_error.pos + 100)
                logger.error(f"Contexte erreur: ...{clean_response[error_context_start:error_context_end]}...")

                # Essayer de r√©parer le JSON
                try:
                    clean_response = self._try_fix_json(clean_response)
                    analysis_data = json.loads(clean_response)
                    logger.info("‚úÖ JSON r√©par√© avec succ√®s")
                except Exception as fix_error:
                    logger.error(f"Impossible de r√©parer le JSON: {fix_error}")
                    logger.warning("Utilisation d'une analyse par d√©faut")
                    return self._create_default_analysis(vulnerabilities_data)

            # V√©rifier la structure
            if not analysis_data:
                logger.error("analysis_data est None apr√®s parsing")
                return self._create_default_analysis(vulnerabilities_data)

            if 'vulnerabilities' not in analysis_data:
                logger.warning("Cl√© 'vulnerabilities' manquante dans la r√©ponse")
                # Si la r√©ponse contient directement une liste, l'utiliser
                if isinstance(analysis_data, list):
                    analysis_data = {'vulnerabilities': analysis_data}
                else:
                    analysis_data = {
                        'vulnerabilities': [],
                        'analysis_summary': analysis_data.get('analysis_summary', {}),
                        'remediation_plan': analysis_data.get('remediation_plan', {})
                    }

            return self._parse_vulnerability_analysis(analysis_data)

        except json.JSONDecodeError as e:
            logger.error(f"Erreur parsing JSON apr√®s tous les essais: {e}")
            logger.error(f"R√©ponse compl√®te (1000 chars): {ai_response[:1000] if ai_response else 'VIDE'}")
            logger.warning("Cr√©ation d'une analyse par d√©faut")
            return self._create_default_analysis(vulnerabilities_data)

        except Exception as e:
            logger.error(f"Erreur inattendue lors du parsing: {e}")
            logger.error(f"Type: {type(e).__name__}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            # Ne pas raise, mais cr√©er une analyse par d√©faut
            logger.warning("Cr√©ation d'une analyse par d√©faut suite √† l'erreur")
            return self._create_default_analysis(vulnerabilities_data)

    def _try_fix_json(self, json_str: str) -> str:
        """Essaie de r√©parer un JSON mal form√©"""
        import re

        logger.debug("Tentative de r√©paration du JSON...")

        # Enlever les commentaires JavaScript
        json_str = re.sub(r'//.*?\n', '\n', json_str)
        json_str = re.sub(r'/\*.*?\*/', '', json_str, flags=re.DOTALL)

        # Remplacer les simples quotes par des doubles
        json_str = json_str.replace("'", '"')

        # Enlever les virgules en trop avant } ou ]
        json_str = re.sub(r',\s*([}\]])', r'\1', json_str)

        # Ajouter des virgules manquantes entre les propri√©t√©s
        json_str = re.sub(r'"\s*\n\s*"', '",\n"', json_str)

        # R√©parer les retours √† la ligne dans les strings
        # Trouver les strings et remplacer les \n par des espaces
        def fix_newlines_in_strings(match):
            return match.group(0).replace('\n', ' ')

        # Ne pas toucher aux \n √©chapp√©s, mais remplacer les \n r√©els dans les strings
        json_str = re.sub(r'"[^"]*"', lambda m: m.group(0).replace('\n', '\\n'), json_str)

        logger.debug(f"JSON apr√®s r√©paration (200 premiers chars): {json_str[:200]}")

        return json_str

    def _create_default_analysis(self, vulnerabilities_data: str) -> List[VulnerabilityAnalysis]:
        """Cr√©e une analyse par d√©faut en cas d'√©chec du parsing"""
        logger.info("Cr√©ation d'une analyse par d√©faut basique")

        vulnerabilities = []
        import re
        cve_pattern = r'CVE-\d{4}-\d{4,}'
        cves = re.findall(cve_pattern, vulnerabilities_data)

        for i, cve in enumerate(cves[:10]):
            vuln = VulnerabilityAnalysis(
                vulnerability_id=cve,
                name=f"Vulnerability {cve}",
                severity="MEDIUM",
                cvss_score=5.0,
                impact_analysis="Analyse automatique indisponible - parsing IA √©chou√©",
                exploitability="UNKNOWN",
                priority_score=50,
                affected_service="Unknown",
                recommended_actions=["Consulter les r√©f√©rences CVE", "Mettre √† jour le service"],
                dependencies=[],
                references=[f"https://nvd.nist.gov/vuln/detail/{cve}"]
            )
            vulnerabilities.append(vuln)

        return vulnerabilities

    def _parse_vulnerability_analysis(self, analysis_data: Dict[str, Any]) -> List[VulnerabilityAnalysis]:
        """Parse les donn√©es d'analyse en objets VulnerabilityAnalysis"""
        vulnerabilities = []

        for vuln_data in analysis_data.get("vulnerabilities", []):
            vulnerability = VulnerabilityAnalysis(
                vulnerability_id=vuln_data.get("id", "unknown"),
                name=vuln_data.get("name", "Unknown"),
                severity=vuln_data.get("severity", "UNKNOWN"),
                cvss_score=float(vuln_data.get("cvss_score", 0.0)),
                impact_analysis=vuln_data.get("impact_analysis", ""),
                exploitability=vuln_data.get("exploitability", "UNKNOWN"),
                priority_score=int(vuln_data.get("priority_score", 5)),
                affected_service=vuln_data.get("affected_service", "Unknown"),
                recommended_actions=vuln_data.get("recommended_actions", []),
                dependencies=vuln_data.get("dependencies", []),
                references=vuln_data.get("references", [])
            )
            vulnerabilities.append(vulnerability)

        return vulnerabilities

    async def _generate_remediation_plan(
            self,
            vulnerabilities: List[VulnerabilityAnalysis],
            business_context: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """G√©n√®re un plan de rem√©diation prioris√©"""

        # Pr√©parer les donn√©es des vuln√©rabilit√©s
        vulns_summary = []
        for vuln in vulnerabilities:
            vulns_summary.append({
                "id": vuln.vulnerability_id,
                "name": vuln.name,
                "severity": vuln.severity,
                "priority_score": vuln.priority_score,
                "recommended_actions": vuln.recommended_actions
            })

        # Contexte business par d√©faut
        if not business_context:
            business_context = {
                "budget_constraints": "Budget limit√©",
                "maintenance_window": "Week-end uniquement",
                "critical_services": "Services web",
                "risk_tolerance": "Faible"
            }

        # Pr√©parer le prompt de priorisation
        prompt = format_priority_assessment_prompt(
            vulnerabilities_list=json.dumps(vulns_summary, indent=2),
            **business_context
        )

        # Analyser avec l'IA
        ai_response = await self._call_ai_model(prompt)

        try:
            return json.loads(ai_response)
        except json.JSONDecodeError:
            logger.warning("Erreur parsing plan rem√©diation, utilisation du plan par d√©faut")
            return self._generate_default_remediation_plan(vulnerabilities)

    def _generate_default_remediation_plan(
            self,
            vulnerabilities: List[VulnerabilityAnalysis]
    ) -> Dict[str, Any]:
        """G√©n√®re un plan de rem√©diation par d√©faut"""

        sorted_vulns = sorted(vulnerabilities, key=lambda v: v.priority_score, reverse=True)

        immediate = [v.vulnerability_id for v in sorted_vulns if v.priority_score >= 8]
        short_term = [v.vulnerability_id for v in sorted_vulns if 5 <= v.priority_score < 8]
        long_term = [v.vulnerability_id for v in sorted_vulns if v.priority_score < 5]

        return {
            "executive_summary": {
                "total_vulnerabilities": len(vulnerabilities),
                "immediate_action_required": len(immediate),
                "estimated_total_effort": f"{len(vulnerabilities) * 2} heures",
                "business_risk_level": "MEDIUM"
            },
            "implementation_roadmap": {
                "phase_1_immediate": {
                    "vulnerabilities": immediate,
                    "duration": "24-48h",
                    "resources_needed": ["Administrateur syst√®me"]
                },
                "phase_2_short_term": {
                    "vulnerabilities": short_term,
                    "duration": "1-2 semaines",
                    "resources_needed": ["√âquipe technique"]
                },
                "phase_3_long_term": {
                    "vulnerabilities": long_term,
                    "duration": "1+ mois",
                    "resources_needed": ["Planification strat√©gique"]
                }
            },
            "recommendations": [
                "Commencer par les vuln√©rabilit√©s critiques",
                "Planifier les fen√™tres de maintenance",
                "Tester les correctifs en environnement de d√©veloppement"
            ]
        }

    async def _generate_analysis_summary(
            self,
            vulnerabilities: List[VulnerabilityAnalysis],
            remediation_plan: Dict[str, Any]
    ) -> Dict[str, Any]:
        """G√©n√®re un r√©sum√© ex√©cutif de l'analyse"""

        total_vulns = len(vulnerabilities)
        severity_counts = {
            "critical": len([v for v in vulnerabilities if v.severity == "CRITICAL"]),
            "high": len([v for v in vulnerabilities if v.severity == "HIGH"]),
            "medium": len([v for v in vulnerabilities if v.severity == "MEDIUM"]),
            "low": len([v for v in vulnerabilities if v.severity == "LOW"])
        }

        if vulnerabilities:
            risk_scores = [v.cvss_score for v in vulnerabilities if v.cvss_score > 0]
            overall_risk = sum(risk_scores) / len(risk_scores) if risk_scores else 0.0
        else:
            overall_risk = 0.0

        return {
            "total_vulnerabilities": total_vulns,
            "critical_count": severity_counts["critical"],
            "high_count": severity_counts["high"],
            "medium_count": severity_counts["medium"],
            "low_count": severity_counts["low"],
            "overall_risk_score": round(overall_risk, 1),
            "immediate_actions_required": remediation_plan.get("executive_summary", {}).get("immediate_action_required",
                                                                                            0),
            "estimated_remediation_time": remediation_plan.get("executive_summary", {}).get("estimated_total_effort",
                                                                                            "Unknown"),
            "top_priority_vulnerabilities": [
                v.vulnerability_id for v in sorted(vulnerabilities, key=lambda x: x.priority_score, reverse=True)[:3]
            ]
        }

    async def _call_ai_model(self, prompt: str) -> str:
        """Appelle le mod√®le IA avec le prompt donn√©"""

        if self.current_provider == "openai":
            return await self._call_openai(prompt)
        elif self.current_provider == "ollama":
            return await self._call_ollama(prompt)
        else:
            raise AnalyzerException(
                f"Fournisseur non support√©: {self.current_provider}",
                CoreErrorCodes.AI_SERVICE_ERROR
            )

    async def _call_openai(self, prompt: str) -> str:
        """Appelle l'API OpenAI"""
        try:
            response = await self.client.chat.completions.create(
                model=self.llm_config.get("model", "gpt-4"),
                messages=[
                    {
                        "role": "system",
                        "content": "Tu es un expert en cybers√©curit√© sp√©cialis√© dans l'analyse de vuln√©rabilit√©s. R√©ponds toujours en JSON valide."
                    },
                    {"role": "user", "content": prompt}
                ],
                max_tokens=self.llm_config.get("max_tokens", 2000),
                temperature=self.llm_config.get("temperature", 0.3),
                timeout=self.llm_config.get("timeout", 60)
            )

            return response.choices[0].message.content

        except openai.APITimeoutError:
            raise AnalyzerException(
                "Timeout de l'API OpenAI",
                CoreErrorCodes.ANALYSIS_TIMEOUT
            )
        except openai.RateLimitError:
            raise AnalyzerException(
                "Quota API OpenAI d√©pass√©",
                CoreErrorCodes.AI_QUOTA_EXCEEDED
            )
        except Exception as e:
            logger.error(f"Erreur API OpenAI: {e}")
            raise AnalyzerException(
                f"Erreur API OpenAI: {str(e)}",
                CoreErrorCodes.AI_SERVICE_ERROR
            )

    async def _call_ollama(self, prompt: str) -> str:
        """Appelle l'API Ollama"""
        try:
            base_url = self.client["base_url"]
            model = self.llm_config.get("model", "llama3")

            data = {
                "model": model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": self.llm_config.get("temperature", 0.3),
                    "num_predict": self.llm_config.get("max_tokens", 2000)
                }
            }

            async with asyncio.timeout(self.llm_config.get("timeout", 60)):
                response = requests.post(
                    f"{base_url}/api/generate",
                    json=data,
                    timeout=30
                )
                response.raise_for_status()

                result = response.json()
                return result.get("response", "")

        except asyncio.TimeoutError:
            raise AnalyzerException(
                "Timeout de l'API Ollama",
                CoreErrorCodes.ANALYSIS_TIMEOUT
            )
        except Exception as e:
            logger.error(f"Erreur API Ollama: {e}")
            raise AnalyzerException(
                f"Erreur API Ollama: {str(e)}",
                CoreErrorCodes.AI_SERVICE_ERROR
            )

    def _calculate_confidence_score(self, vulnerabilities: List[VulnerabilityAnalysis]) -> float:
        """Calcule un score de confiance pour l'analyse"""
        if not vulnerabilities:
            return 0.0

        factors = []

        cve_ratio = len([v for v in vulnerabilities if v.vulnerability_id.startswith("CVE")]) / len(vulnerabilities)
        factors.append(cve_ratio * 0.3)

        priority_scores = [v.priority_score for v in vulnerabilities]
        priority_variance = 1.0 - (max(priority_scores) - min(priority_scores)) / 10.0 if priority_scores else 0.0
        factors.append(max(0.0, priority_variance) * 0.2)

        reco_ratio = len([v for v in vulnerabilities if v.recommended_actions]) / len(vulnerabilities)
        factors.append(reco_ratio * 0.3)

        factors.append(0.2)

        return min(1.0, sum(factors))

    def _get_model_name(self) -> str:
        """Retourne le nom du mod√®le IA utilis√©"""
        if self.current_provider == "openai":
            return self.llm_config.get("model", "gpt-4")
        elif self.current_provider == "ollama":
            return f"ollama:{self.llm_config.get('model', 'llama3')}"
        else:
            return "unknown"

    async def _save_analysis_result(self, result: AnalysisResult):
        """Sauvegarde le r√©sultat d'analyse dans la base de donn√©es"""
        try:
            logger.debug(f"Sauvegarde analyse: {result.analysis_id}")
        except Exception as e:
            logger.warning(f"Erreur sauvegarde analyse: {e}")

    def _update_stats(self, success: bool, processing_time: float):
        """Met √† jour les statistiques de l'analyseur"""
        self.stats["total_analyses"] += 1

        if success:
            self.stats["successful_analyses"] += 1
        else:
            self.stats["failed_analyses"] += 1

        current_avg = self.stats["average_processing_time"]
        total = self.stats["total_analyses"]
        self.stats["average_processing_time"] = (current_avg * (total - 1) + processing_time) / total

    def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques de l'analyseur"""
        return self.stats.copy()

    def is_healthy(self) -> bool:
        """V√©rifie si l'analyseur est en bonne sant√©"""
        if not self.is_ready:
            return False

        try:
            if self.current_provider == "openai":
                return True
            elif self.current_provider == "ollama":
                base_url = self.client["base_url"]
                response = requests.get(f"{base_url}/api/tags", timeout=5)
                return response.status_code == 200

            return True

        except Exception:
            return False


# === FONCTIONS UTILITAIRES ===

async def quick_vulnerability_analysis(
        vulnerabilities: List[Dict[str, Any]],
        target_system: str = "Unknown System"
) -> Dict[str, Any]:
    """
    Analyse rapide de vuln√©rabilit√©s (fonction utilitaire)

    Args:
        vulnerabilities: Liste des vuln√©rabilit√©s
        target_system: Syst√®me cible

    Returns:
        Dict contenant l'analyse simplifi√©e
    """
    analyzer = Analyzer()

    try:
        result = await analyzer.analyze_vulnerabilities(vulnerabilities, target_system)
        return result.to_dict()
    except Exception as e:
        logger.error(f"Erreur analyse rapide: {e}")
        return {
            "error": str(e),
            "analysis_summary": {
                "total_vulnerabilities": len(vulnerabilities),
                "overall_risk_score": 0.0
            }
        }


def create_analyzer(provider: str = "openai") -> Analyzer:
    """
    Factory pour cr√©er un analyseur avec un provider sp√©cifique

    Args:
        provider: Fournisseur IA (openai, ollama, anthropic)

    Returns:
        Analyzer: Instance configur√©e
    """
    config = get_llm_config(provider)
    return Analyzer(config)