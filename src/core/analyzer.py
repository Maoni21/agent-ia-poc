"""
Module Analyzer pour l'Agent IA de CybersÃ©curitÃ©
Support OpenAI ET Anthropic/Claude
"""

import asyncio
import json
import logging
import time
import uuid
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict

from config import get_config
from src.utils.logger import setup_logger
from src.database.database import Database
from .exceptions import AnalyzerException, CoreErrorCodes
from .false_positive_detector import FalsePositiveDetector

logger = setup_logger(__name__)


# === MODÃˆLES DE DONNÃ‰ES ===

@dataclass
class VulnerabilityAnalysis:
    """Analyse complÃ¨te d'une vulnÃ©rabilitÃ©"""
    vulnerability_id: str
    name: str
    severity: str
    cvss_score: float
    impact_analysis: str
    exploitability: str
    priority_score: int
    affected_service: str
    recommended_actions: List[str]
    dependencies: List[str]
    references: List[str]
    cvss_vector: Optional[str] = None
    nist_verified: bool = False
    nist_url: Optional[str] = None
    solution_links: List[str] = None
    ai_explanation: Optional[str] = None
    correction_script: Optional[str] = None
    rollback_script: Optional[str] = None
    business_impact: Optional[str] = None
    is_false_positive: bool = False
    false_positive_confidence: Optional[float] = None
    false_positive_reasoning: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


@dataclass
class AnalysisResult:
    """RÃ©sultat complet d'une analyse"""
    analysis_id: str
    target_system: str
    analyzed_at: datetime
    analysis_summary: Dict[str, Any]
    vulnerabilities: List[VulnerabilityAnalysis]
    remediation_plan: Dict[str, Any]
    ai_model_used: str
    confidence_score: float
    processing_time: float
    business_context: Optional[str] = None
    nist_enriched: bool = False
    nist_call_count: int = 0
    nist_cache_hits: int = 0

    def to_dict(self) -> Dict[str, Any]:
        result = asdict(self)
        result['analyzed_at'] = self.analyzed_at.isoformat()
        result['vulnerabilities'] = [vuln.to_dict() for vuln in self.vulnerabilities]
        return result


# === CLASSE PRINCIPALE ===

class Analyzer:
    """Analyseur IA de vulnÃ©rabilitÃ©s avec support multi-provider"""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or get_config()
        self.db = Database()
        self.is_ready = False
        self.stats = {
            "total_analyses": 0,
            "successful_analyses": 0,
            "failed_analyses": 0,
            "average_processing_time": 0.0,
            "nist_api_calls": 0,
            "nist_cache_hits": 0,
            "false_positives_detected": 0
        }

        # DÃ©tection du provider
        self.ai_provider = self.config.get('ai_provider', 'anthropic')

        # Initialisation du client IA selon le provider
        if self.ai_provider == 'openai':
            self._init_openai()
        elif self.ai_provider == 'anthropic':
            self._init_anthropic()
        else:
            raise AnalyzerException(f"Provider IA non supportÃ©: {self.ai_provider}", CoreErrorCodes.CORE_INIT_ERROR)
        
        # Initialiser le dÃ©tecteur de faux positifs
        try:
            self.false_positive_detector = FalsePositiveDetector(self.config)
            logger.info("âœ… FalsePositiveDetector initialisÃ©")
        except Exception as e:
            logger.warning(f"âš ï¸ FalsePositiveDetector non disponible: {e}")
            self.false_positive_detector = None

    def _init_openai(self):
        """Initialise le client OpenAI"""
        try:
            from openai import AsyncOpenAI
            self.client = AsyncOpenAI(
                api_key=self.config.get('openai_api_key'),
                timeout=self.config.get('openai_timeout', 120)
            )
            self.model = self.config.get('openai_model', 'gpt-4')
            self.max_tokens = self.config.get('openai_max_tokens', 1000)
            self.temperature = self.config.get('openai_temperature', 0.7)
            self.is_ready = True
            logger.info(f"Client OpenAI initialisÃ© (modÃ¨le: {self.model})")
        except Exception as e:
            logger.error(f"Erreur initialisation OpenAI: {e}")
            raise AnalyzerException(f"Impossible d'initialiser OpenAI: {str(e)}", CoreErrorCodes.CORE_INIT_ERROR)

    def _init_anthropic(self):
        """Initialise le client Anthropic/Claude"""
        try:
            from anthropic import AsyncAnthropic
            self.client = AsyncAnthropic(
                api_key=self.config.get('anthropic_api_key'),
                timeout=self.config.get('anthropic_timeout', 120)
            )
            self.model = self.config.get('anthropic_model', 'claude-sonnet-4-20250514')
            self.max_tokens = self.config.get('anthropic_max_tokens', 4096)
            self.temperature = self.config.get('anthropic_temperature', 0.7)
            self.is_ready = True
            logger.info(f"Client Anthropic initialisÃ© (modÃ¨le: {self.model})")
        except Exception as e:
            logger.error(f"Erreur initialisation Anthropic: {e}")
            raise AnalyzerException(f"Impossible d'initialiser Anthropic: {str(e)}", CoreErrorCodes.CORE_INIT_ERROR)

        logger.info("âœ… Analyzer initialisÃ© (NIST + IA + Parser robuste)")

    async def _call_ai_api(self, prompt: str) -> str:
        """Appelle l'API IA selon le provider configurÃ©"""
        if self.ai_provider == 'openai':
            return await self._call_openai(prompt)
        elif self.ai_provider == 'anthropic':
            return await self._call_anthropic(prompt)
        else:
            raise AnalyzerException(f"Provider inconnu: {self.ai_provider}", CoreErrorCodes.AI_SERVICE_ERROR)

    async def _call_openai(self, prompt: str) -> str:
        """Appelle l'API OpenAI"""
        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "Tu es un expert en cybersÃ©curitÃ©. RÃ©ponds UNIQUEMENT en JSON valide."},
                {"role": "user", "content": prompt}
            ],
            temperature=self.temperature,
            max_tokens=self.max_tokens
        )
        return response.choices[0].message.content

    async def _call_anthropic(self, prompt: str) -> str:
        """Appelle l'API Anthropic/Claude"""
        response = await self.client.messages.create(
            model=self.model,
            max_tokens=self.max_tokens,
            temperature=self.temperature,
            system="Tu es un expert en cybersÃ©curitÃ©. RÃ©ponds UNIQUEMENT en JSON valide.",
            messages=[
                {"role": "user", "content": prompt}
            ]
        )
        return response.content[0].text

    async def analyze_vulnerabilities_batch(
            self,
            vulnerabilities_data: List[Dict[str, Any]],
            target_system: str = "Unknown System",
            business_context: Optional[str] = None,
            batch_size: int = 10
    ) -> AnalysisResult:
        """Analyse un batch de vulnÃ©rabilitÃ©s"""

        if not vulnerabilities_data:
            raise AnalyzerException("Aucune vulnÃ©rabilitÃ© Ã  analyser", CoreErrorCodes.INVALID_VULNERABILITY_DATA)

        start_time = time.time()
        analysis_id = f"analysis_batch_{int(time.time())}"

        logger.info("ğŸ”§ Normalisation des clÃ©s...")
        vulnerabilities_data = self._normalize_vulnerability_keys(vulnerabilities_data)

        logger.info(f"ğŸ” Analyse par batch: {len(vulnerabilities_data)} vulns, batch_size={batch_size}")

        batches = [vulnerabilities_data[i:i + batch_size]
                   for i in range(0, len(vulnerabilities_data), batch_size)]

        logger.info(f"ğŸ“¦ {len(batches)} batches Ã  traiter")

        all_analyzed_vulns = []

        for i, batch in enumerate(batches, 1):
            logger.info(f"ğŸ”„ Batch {i}/{len(batches)} ({len(batch)} vulns)")

            analyzed_vulns = await self._analyze_batch(
                batch,
                target_system,
                business_context
            )

            all_analyzed_vulns.extend(analyzed_vulns)
            logger.info(f"âœ… Batch {i}/{len(batches)} terminÃ©")

            if i < len(batches):
                await asyncio.sleep(1)

        logger.info("ğŸ“‹ Plan de remÃ©diation global...")
        remediation_plan = self._create_remediation_plan(all_analyzed_vulns)

        processing_time = time.time() - start_time
        logger.info(f"âœ… Analyse batch terminÃ©e en {processing_time:.2f}s")

        analysis_summary = self._create_analysis_summary(all_analyzed_vulns)

        result = AnalysisResult(
            analysis_id=analysis_id,
            target_system=target_system,
            analyzed_at=datetime.utcnow(),
            analysis_summary=analysis_summary,
            vulnerabilities=all_analyzed_vulns,
            remediation_plan=remediation_plan,
            ai_model_used=self.model,
            confidence_score=self._calculate_confidence_score(all_analyzed_vulns),
            processing_time=processing_time,
            business_context=business_context,
            nist_enriched=True,
            nist_call_count=self.stats["nist_api_calls"],
            nist_cache_hits=self.stats["nist_cache_hits"]
        )

        self._update_stats(True, processing_time)
        return result

    async def _analyze_batch(
            self,
            vulnerabilities: List[Dict[str, Any]],
            target_system: str,
            business_context: Optional[str]
    ) -> List[VulnerabilityAnalysis]:
        """Analyse un batch de vulnÃ©rabilitÃ©s"""

        logger.info(f"ğŸ” Analyse: {len(vulnerabilities)} vulnÃ©rabilitÃ©s")

        logger.info("ğŸ“Š Enrichissement NIST...")
        enriched_vulns = await self._enrich_with_nist_batch(vulnerabilities)

        logger.info("ğŸ¤– Analyse IA...")
        analyzed_vulns = await self._ai_analysis(enriched_vulns, target_system, business_context)

        # DÃ©tection de faux positifs (si activÃ©)
        if self.false_positive_detector and self.config.get('enable_false_positive_detection', True):
            logger.info("ğŸ” DÃ©tection de faux positifs...")
            analyzed_vulns = await self._detect_false_positives(analyzed_vulns, target_system)

        return analyzed_vulns

    async def _enrich_with_nist_batch(self, vulnerabilities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:

        from src.core.nist_enricher import NISTEnricher

        nist_client = NISTEnricher()
        enriched = []

        for vuln in vulnerabilities:
            cve_id = vuln.get('cve_id') or vuln.get('vulnerability_id', '')

            if cve_id and cve_id.startswith('CVE-'):
                nist_data = await nist_client.enrich_cve(cve_id)

                if nist_data:
                    self.stats["nist_cache_hits"] += 1
                    vuln['nist_data'] = nist_data
                    vuln['nist_verified'] = True
                    vuln['nist_url'] = f"https://nvd.nist.gov/vuln/detail/{cve_id}"

                    if 'cvss_v3' in nist_data.get('metrics', {}):
                        cvss_data = nist_data['metrics']['cvss_v3'][0]['cvssData']
                        vuln['cvss_score'] = cvss_data.get('baseScore', vuln.get('cvss_score', 0.0))
                        vuln['cvss_vector'] = cvss_data.get('vectorString')
                        vuln['severity'] = cvss_data.get('baseSeverity', vuln.get('severity', 'UNKNOWN'))

                    if 'references' in nist_data:
                        vuln['solution_links'] = [
                            ref['url'] for ref in nist_data['references'][:3]
                            if 'url' in ref
                        ]

            enriched.append(vuln)

        return enriched

    async def _ai_analysis(
            self,
            vulnerabilities: List[Dict[str, Any]],
            target_system: str,
            business_context: Optional[str]
    ) -> List[VulnerabilityAnalysis]:
        """Analyse IA des vulnÃ©rabilitÃ©s avec retry"""

        max_retries = 2

        for attempt in range(1, max_retries + 1):
            try:
                if attempt == 1:
                    prompt = self._build_analysis_prompt(vulnerabilities, target_system, business_context)
                else:
                    logger.info("ğŸ”„ Retry avec prompt simplifiÃ©...")
                    prompt = self._build_simplified_prompt(vulnerabilities, target_system)

                response_text = await self._call_ai_api(prompt)

                response_text = response_text.strip()
                if response_text.startswith('```json'):
                    response_text = response_text[7:]
                if response_text.startswith('```'):
                    response_text = response_text[3:]
                if response_text.endswith('```'):
                    response_text = response_text[:-3]
                response_text = response_text.strip()

                try:
                    analysis_data = json.loads(response_text)
                    logger.info(f"âœ… JSON parsÃ©: {len(analysis_data.get('vulnerabilities', []))} vulnÃ©rabilitÃ©s")
                except json.JSONDecodeError as e:
                    logger.error(f"âŒ JSON invalide: {e}")
                    raise

                analyzed_vulns = []
                for vuln_data in analysis_data.get('vulnerabilities', []):
                    original_vuln = next(
                        (v for v in vulnerabilities
                         if v.get('vulnerability_id') == vuln_data.get('vulnerability_id')),
                        {}
                    )

                    vuln = VulnerabilityAnalysis(
                        vulnerability_id=vuln_data.get('vulnerability_id', 'UNKNOWN'),
                        name=vuln_data.get('name', 'Unknown'),
                        severity=vuln_data.get('severity', 'UNKNOWN'),
                        cvss_score=float(vuln_data.get('cvss_score', 0.0)),
                        impact_analysis=vuln_data.get('impact_analysis', ''),
                        exploitability=vuln_data.get('exploitability', 'UNKNOWN'),
                        priority_score=int(vuln_data.get('priority_score', 5)),
                        affected_service=vuln_data.get('affected_service', 'Unknown'),
                        recommended_actions=vuln_data.get('recommended_actions', []),
                        dependencies=vuln_data.get('dependencies', []),
                        references=vuln_data.get('references', []),
                        cvss_vector=original_vuln.get('cvss_vector'),
                        nist_verified=original_vuln.get('nist_verified', False),
                        nist_url=original_vuln.get('nist_url'),
                        solution_links=original_vuln.get('solution_links', []),
                        ai_explanation=vuln_data.get('ai_explanation'),
                        business_impact=vuln_data.get('business_impact')
                    )
                    analyzed_vulns.append(vuln)

                logger.info(f"âœ… Analyse terminÃ©e en {time.time():.2f}s")
                return analyzed_vulns

            except json.JSONDecodeError as e:
                logger.warning(f"Tentative {attempt} Ã©chouÃ©e: {e}")
                if attempt == max_retries:
                    raise AnalyzerException(f"JSON invalide aprÃ¨s {max_retries} tentatives",
                                            CoreErrorCodes.AI_RESPONSE_PARSE_ERROR)
            except Exception as e:
                logger.error(f"Erreur tentative {attempt}: {e}")
                if attempt == max_retries:
                    raise

    def _build_analysis_prompt(self, vulnerabilities: List[Dict], target_system: str,
                               business_context: Optional[str]) -> str:
        """Construit le prompt d'analyse (version complÃ¨te)"""
        vulns_json = json.dumps([{
            'vulnerability_id': v.get('vulnerability_id', v.get('cve_id', 'UNKNOWN')),
            'name': v.get('name', v.get('vulnerability_name', 'Unknown')),
            'severity': v.get('severity', 'UNKNOWN'),
            'cvss_score': v.get('cvss_score', 0.0),
            'affected_service': v.get('service', v.get('affected_service', 'Unknown')),
            'description': v.get('description', '')[:200]
        } for v in vulnerabilities], indent=2)

        prompt = f"""Analyse ces vulnÃ©rabilitÃ©s pour le systÃ¨me: {target_system}

VulnÃ©rabilitÃ©s Ã  analyser:
{vulns_json}

RÃ©ponds UNIQUEMENT avec un JSON valide (pas de markdown, pas de texte) au format:
{{
  "vulnerabilities": [
    {{
      "vulnerability_id": "CVE-XXXX-XXXX",
      "name": "Nom de la vulnÃ©rabilitÃ©",
      "severity": "CRITICAL|HIGH|MEDIUM|LOW",
      "cvss_score": 9.8,
      "impact_analysis": "Description de l'impact",
      "exploitability": "HIGH|MEDIUM|LOW",
      "priority_score": 10,
      "affected_service": "nom_du_service",
      "recommended_actions": ["Action 1", "Action 2"],
      "dependencies": [],
      "references": [],
      "ai_explanation": "Explication dÃ©taillÃ©e",
      "business_impact": "Impact business"
    }}
  ]
}}

IMPORTANT: RÃ©ponds UNIQUEMENT avec le JSON, sans texte avant ou aprÃ¨s."""

        return prompt

    def _build_simplified_prompt(self, vulnerabilities: List[Dict], target_system: str) -> str:
        """Construit un prompt simplifiÃ© pour retry"""
        vulns_simple = [
            f"- {v.get('vulnerability_id', 'UNKNOWN')}: {v.get('name', 'Unknown')} (GravitÃ©: {v.get('severity', 'UNKNOWN')})"
            for v in vulnerabilities
        ]

        prompt = f"""SystÃ¨me: {target_system}

VulnÃ©rabilitÃ©s:
{chr(10).join(vulns_simple)}

JSON uniquement:
{{"vulnerabilities": [...]}}"""

        return prompt

    def _normalize_vulnerability_keys(self, vulnerabilities: List[Dict]) -> List[Dict]:
        """Normalise les clÃ©s des vulnÃ©rabilitÃ©s"""
        normalized = []

        for vuln in vulnerabilities:
            normalized_vuln = {
                'vulnerability_id': vuln.get('vulnerability_id') or vuln.get('cve_id', 'UNKNOWN'),
                'name': vuln.get('name') or vuln.get('vulnerability_name', 'Unknown'),
                'severity': vuln.get('severity', 'UNKNOWN'),
                'cvss_score': vuln.get('cvss_score', 0.0),
                'service': vuln.get('service') or vuln.get('affected_service', 'Unknown'),
                'description': vuln.get('description', ''),
                'port': vuln.get('port'),
                'protocol': vuln.get('protocol')
            }
            normalized.append(normalized_vuln)

        return normalized

    def _create_remediation_plan(self, vulnerabilities: List[VulnerabilityAnalysis]) -> Dict[str, Any]:
        """CrÃ©e un plan de remÃ©diation"""
        critical_vulns = [v for v in vulnerabilities if v.severity == "CRITICAL"]
        high_vulns = [v for v in vulnerabilities if v.severity == "HIGH"]

        immediate_actions = []
        for vuln in sorted(critical_vulns + high_vulns, key=lambda x: x.priority_score, reverse=True):
            if vuln.recommended_actions:
                immediate_actions.append({
                    "vulnerability_id": vuln.vulnerability_id,
                    "action": vuln.recommended_actions[0],
                    "priority": vuln.priority_score,
                    "estimated_time": self._estimate_fix_time(vuln)
                })

        medium_vulns = [v for v in vulnerabilities if v.severity == "MEDIUM"]
        short_term_actions = []
        for vuln in medium_vulns:
            if vuln.recommended_actions:
                short_term_actions.append({
                    "vulnerability_id": vuln.vulnerability_id,
                    "action": vuln.recommended_actions[0],
                    "priority": vuln.priority_score
                })

        return {
            "immediate_actions": immediate_actions[:10],
            "short_term_actions": short_term_actions[:10],
            "long_term_actions": [],
            "estimated_total_time_hours": sum(a.get('estimated_time', 1) for a in immediate_actions),
            "critical_count": len(critical_vulns),
            "high_count": len(high_vulns),
            "priority_order": [v.vulnerability_id for v in
                               sorted(vulnerabilities, key=lambda x: x.priority_score, reverse=True)]
        }

    def _estimate_fix_time(self, vuln: VulnerabilityAnalysis) -> float:
        """Estime le temps de correction"""
        base_times = {
            "CRITICAL": 4.0,
            "HIGH": 2.0,
            "MEDIUM": 1.0,
            "LOW": 0.5
        }
        return base_times.get(vuln.severity, 1.0)

    def _create_analysis_summary(self, vulnerabilities: List[VulnerabilityAnalysis]) -> Dict[str, Any]:
        """CrÃ©e un rÃ©sumÃ© de l'analyse"""
        severity_counts = {"CRITICAL": 0, "HIGH": 0, "MEDIUM": 0, "LOW": 0, "UNKNOWN": 0}

        for vuln in vulnerabilities:
            severity_counts[vuln.severity] = severity_counts.get(vuln.severity, 0) + 1

        cvss_scores = [v.cvss_score for v in vulnerabilities if v.cvss_score > 0]
        avg_cvss = sum(cvss_scores) / len(cvss_scores) if cvss_scores else 0.0

        priorities = [v.priority_score for v in vulnerabilities]
        max_priority = max(priorities) if priorities else 0

        nist_verified = sum(1 for v in vulnerabilities if v.nist_verified)

        risk_score = (
                             severity_counts["CRITICAL"] * 10 +
                             severity_counts["HIGH"] * 5 +
                             severity_counts["MEDIUM"] * 2 +
                             severity_counts["LOW"] * 1
                     ) / max(len(vulnerabilities), 1)

        return {
            "total_vulnerabilities": len(vulnerabilities),
            "critical_count": severity_counts["CRITICAL"],
            "high_count": severity_counts["HIGH"],
            "medium_count": severity_counts["MEDIUM"],
            "low_count": severity_counts["LOW"],
            "unknown_count": severity_counts["UNKNOWN"],
            "overall_risk_score": round(risk_score, 1),
            "average_cvss": round(avg_cvss, 1),
            "highest_priority": max_priority,
            "nist_verified_count": nist_verified
        }

    def _calculate_confidence_score(self, vulnerabilities: List[VulnerabilityAnalysis]) -> float:
        """Calcule un score de confiance"""
        if not vulnerabilities:
            return 0.0

        nist_verified_count = sum(1 for v in vulnerabilities if v.nist_verified)
        nist_ratio = nist_verified_count / len(vulnerabilities)

        has_recommendations = sum(1 for v in vulnerabilities if v.recommended_actions)
        recommendations_ratio = has_recommendations / len(vulnerabilities)

        confidence = (nist_ratio * 0.6) + (recommendations_ratio * 0.4)
        return round(confidence, 2)

    def _update_stats(self, success: bool, processing_time: float):
        """Met Ã  jour les statistiques"""
        self.stats["total_analyses"] += 1
        if success:
            self.stats["successful_analyses"] += 1
        else:
            self.stats["failed_analyses"] += 1

        current_avg = self.stats["average_processing_time"]
        total = self.stats["total_analyses"]
        self.stats["average_processing_time"] = (current_avg * (total - 1) + processing_time) / total

    async def _detect_false_positives(
            self,
            analyzed_vulns: List[VulnerabilityAnalysis],
            target_system: str
    ) -> List[VulnerabilityAnalysis]:
        """DÃ©tecte les faux positifs dans les vulnÃ©rabilitÃ©s analysÃ©es"""
        if not self.false_positive_detector:
            return analyzed_vulns
        
        # Construire le contexte de scan (basique pour l'instant)
        scan_context = {
            "scan_type": "full",
            "open_ports": [],
            "banners": {},
            "os": target_system
        }
        
        # Analyser chaque vulnÃ©rabilitÃ©
        for vuln in analyzed_vulns:
            try:
                vuln_dict = vuln.to_dict()
                fp_analysis = await self.false_positive_detector.analyze_vulnerability(
                    vuln_dict,
                    scan_context
                )
                
                # Mettre Ã  jour la vulnÃ©rabilitÃ© avec les rÃ©sultats
                vuln.is_false_positive = fp_analysis.is_false_positive
                vuln.false_positive_confidence = fp_analysis.confidence
                vuln.false_positive_reasoning = fp_analysis.reasoning
                
                # Mettre Ã  jour les stats
                if fp_analysis.is_false_positive:
                    self.stats["false_positives_detected"] += 1
                    logger.info(f"âš ï¸ Faux positif dÃ©tectÃ©: {vuln.vulnerability_id} (confiance: {fp_analysis.confidence:.2f})")
                
                # Petite pause pour Ã©viter le rate limiting
                await asyncio.sleep(0.5)
                
            except Exception as e:
                logger.warning(f"Erreur dÃ©tection faux positif pour {vuln.vulnerability_id}: {e}")
                continue
        
        return analyzed_vulns

    def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques"""
        return {**self.stats, "provider": self.ai_provider, "model": self.model}

    def is_healthy(self) -> bool:
        """VÃ©rifie si l'analyseur est en bonne santÃ©"""
        return self.is_ready